{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18544090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f0946e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Aashish'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "452c50ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 Load the dataset\n",
    "df = pd.read_csv(\"renttherunway.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31f67a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few samples:\n",
      "   Unnamed: 0  fit  user_id bust size  item_id  weight  rating     rented for  \\\n",
      "0           0  fit   420272       34d  2260466  137lbs    10.0       vacation   \n",
      "1           1  fit   273551       34b   153475  132lbs    10.0          other   \n",
      "2           2  fit   360448       NaN  1063761     NaN    10.0          party   \n",
      "3           3  fit   909926       34c   126335  135lbs     8.0  formal affair   \n",
      "4           4  fit   151944       34b   616682  145lbs    10.0        wedding   \n",
      "\n",
      "                                         review_text          body type  \\\n",
      "0  An adorable romper! Belt and zipper were a lit...          hourglass   \n",
      "1  I rented this dress for a photo shoot. The the...  straight & narrow   \n",
      "2  This hugged in all the right places! It was a ...                NaN   \n",
      "3  I rented this for my company's black tie award...               pear   \n",
      "4  I have always been petite in my upper body and...           athletic   \n",
      "\n",
      "                                      review_summary category height  size  \\\n",
      "0                               So many compliments!   romper  5' 8\"    14   \n",
      "1                            I felt so glamourous!!!     gown  5' 6\"    12   \n",
      "2  It was a great time to celebrate the (almost) ...   sheath  5' 4\"     4   \n",
      "3   Dress arrived on time and in perfect condition.     dress  5' 5\"     8   \n",
      "4                    Was in love with this dress !!!     gown  5' 9\"    12   \n",
      "\n",
      "     age         review_date  \n",
      "0   28.0      April 20, 2016  \n",
      "1   36.0       June 18, 2013  \n",
      "2  116.0   December 14, 2015  \n",
      "3   34.0   February 12, 2014  \n",
      "4   27.0  September 26, 2016  \n"
     ]
    }
   ],
   "source": [
    "#2 Displaying the first few samples\n",
    "print(\"First few samples:\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1192832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shape of the data:\n",
      "(192544, 16)\n"
     ]
    }
   ],
   "source": [
    "# Checking the shape of the data\n",
    "print(\"\\nShape of the data:\")\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abef8486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Information about the data:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 192544 entries, 0 to 192543\n",
      "Data columns (total 16 columns):\n",
      " #   Column          Non-Null Count   Dtype  \n",
      "---  ------          --------------   -----  \n",
      " 0   Unnamed: 0      192544 non-null  int64  \n",
      " 1   fit             192544 non-null  object \n",
      " 2   user_id         192544 non-null  int64  \n",
      " 3   bust size       174133 non-null  object \n",
      " 4   item_id         192544 non-null  int64  \n",
      " 5   weight          162562 non-null  object \n",
      " 6   rating          192462 non-null  float64\n",
      " 7   rented for      192534 non-null  object \n",
      " 8   review_text     192476 non-null  object \n",
      " 9   body type       177907 non-null  object \n",
      " 10  review_summary  192197 non-null  object \n",
      " 11  category        192544 non-null  object \n",
      " 12  height          191867 non-null  object \n",
      " 13  size            192544 non-null  int64  \n",
      " 14  age             191584 non-null  float64\n",
      " 15  review_date     192544 non-null  object \n",
      "dtypes: float64(2), int64(4), object(10)\n",
      "memory usage: 23.5+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Getting information about the data\n",
    "print(\"\\nInformation about the data:\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea57392e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No duplicate records found in the dataset.\n"
     ]
    }
   ],
   "source": [
    "#3 Check for duplicate records\n",
    "duplicate_count = df.duplicated().sum()\n",
    "\n",
    "# If duplicates are found, drop them\n",
    "if duplicate_count > 0:\n",
    "    print(f\"There are {duplicate_count} duplicate records in the dataset. Dropping duplicates...\")\n",
    "    df.drop_duplicates(inplace=True)\n",
    "else:\n",
    "    print(\"No duplicate records found in the dataset.\")\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dac18fe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>fit</th>\n",
       "      <th>user_id</th>\n",
       "      <th>bust size</th>\n",
       "      <th>item_id</th>\n",
       "      <th>weight</th>\n",
       "      <th>rating</th>\n",
       "      <th>rented for</th>\n",
       "      <th>review_text</th>\n",
       "      <th>body type</th>\n",
       "      <th>review_summary</th>\n",
       "      <th>category</th>\n",
       "      <th>height</th>\n",
       "      <th>size</th>\n",
       "      <th>age</th>\n",
       "      <th>review_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>fit</td>\n",
       "      <td>420272</td>\n",
       "      <td>34d</td>\n",
       "      <td>2260466</td>\n",
       "      <td>137lbs</td>\n",
       "      <td>10.0</td>\n",
       "      <td>vacation</td>\n",
       "      <td>An adorable romper! Belt and zipper were a lit...</td>\n",
       "      <td>hourglass</td>\n",
       "      <td>So many compliments!</td>\n",
       "      <td>romper</td>\n",
       "      <td>5' 8\"</td>\n",
       "      <td>14</td>\n",
       "      <td>28.0</td>\n",
       "      <td>April 20, 2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>fit</td>\n",
       "      <td>273551</td>\n",
       "      <td>34b</td>\n",
       "      <td>153475</td>\n",
       "      <td>132lbs</td>\n",
       "      <td>10.0</td>\n",
       "      <td>other</td>\n",
       "      <td>I rented this dress for a photo shoot. The the...</td>\n",
       "      <td>straight &amp; narrow</td>\n",
       "      <td>I felt so glamourous!!!</td>\n",
       "      <td>gown</td>\n",
       "      <td>5' 6\"</td>\n",
       "      <td>12</td>\n",
       "      <td>36.0</td>\n",
       "      <td>June 18, 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>fit</td>\n",
       "      <td>360448</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1063761</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>party</td>\n",
       "      <td>This hugged in all the right places! It was a ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>It was a great time to celebrate the (almost) ...</td>\n",
       "      <td>sheath</td>\n",
       "      <td>5' 4\"</td>\n",
       "      <td>4</td>\n",
       "      <td>116.0</td>\n",
       "      <td>December 14, 2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>fit</td>\n",
       "      <td>909926</td>\n",
       "      <td>34c</td>\n",
       "      <td>126335</td>\n",
       "      <td>135lbs</td>\n",
       "      <td>8.0</td>\n",
       "      <td>formal affair</td>\n",
       "      <td>I rented this for my company's black tie award...</td>\n",
       "      <td>pear</td>\n",
       "      <td>Dress arrived on time and in perfect condition.</td>\n",
       "      <td>dress</td>\n",
       "      <td>5' 5\"</td>\n",
       "      <td>8</td>\n",
       "      <td>34.0</td>\n",
       "      <td>February 12, 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>fit</td>\n",
       "      <td>151944</td>\n",
       "      <td>34b</td>\n",
       "      <td>616682</td>\n",
       "      <td>145lbs</td>\n",
       "      <td>10.0</td>\n",
       "      <td>wedding</td>\n",
       "      <td>I have always been petite in my upper body and...</td>\n",
       "      <td>athletic</td>\n",
       "      <td>Was in love with this dress !!!</td>\n",
       "      <td>gown</td>\n",
       "      <td>5' 9\"</td>\n",
       "      <td>12</td>\n",
       "      <td>27.0</td>\n",
       "      <td>September 26, 2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192539</th>\n",
       "      <td>192539</td>\n",
       "      <td>fit</td>\n",
       "      <td>66386</td>\n",
       "      <td>34dd</td>\n",
       "      <td>2252812</td>\n",
       "      <td>140lbs</td>\n",
       "      <td>10.0</td>\n",
       "      <td>work</td>\n",
       "      <td>Fit like a glove!</td>\n",
       "      <td>hourglass</td>\n",
       "      <td>LOVE IT!!! First Item Im thinking of buying!</td>\n",
       "      <td>jumpsuit</td>\n",
       "      <td>5' 9\"</td>\n",
       "      <td>8</td>\n",
       "      <td>42.0</td>\n",
       "      <td>May 18, 2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192540</th>\n",
       "      <td>192540</td>\n",
       "      <td>fit</td>\n",
       "      <td>118398</td>\n",
       "      <td>32c</td>\n",
       "      <td>682043</td>\n",
       "      <td>100lbs</td>\n",
       "      <td>10.0</td>\n",
       "      <td>work</td>\n",
       "      <td>The pattern contrast on this dress is really s...</td>\n",
       "      <td>petite</td>\n",
       "      <td>LOVE it!</td>\n",
       "      <td>dress</td>\n",
       "      <td>5' 1\"</td>\n",
       "      <td>4</td>\n",
       "      <td>29.0</td>\n",
       "      <td>September 30, 2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192541</th>\n",
       "      <td>192541</td>\n",
       "      <td>fit</td>\n",
       "      <td>47002</td>\n",
       "      <td>36a</td>\n",
       "      <td>683251</td>\n",
       "      <td>135lbs</td>\n",
       "      <td>6.0</td>\n",
       "      <td>everyday</td>\n",
       "      <td>Like the other DVF wraps, the fit on this is f...</td>\n",
       "      <td>straight &amp; narrow</td>\n",
       "      <td>Loud patterning, flattering fit</td>\n",
       "      <td>dress</td>\n",
       "      <td>5' 8\"</td>\n",
       "      <td>8</td>\n",
       "      <td>31.0</td>\n",
       "      <td>March 4, 2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192542</th>\n",
       "      <td>192542</td>\n",
       "      <td>fit</td>\n",
       "      <td>961120</td>\n",
       "      <td>36c</td>\n",
       "      <td>126335</td>\n",
       "      <td>165lbs</td>\n",
       "      <td>10.0</td>\n",
       "      <td>wedding</td>\n",
       "      <td>This dress was PERFECTION.  it looked incredib...</td>\n",
       "      <td>pear</td>\n",
       "      <td>loved this dress it was comfortable and photog...</td>\n",
       "      <td>dress</td>\n",
       "      <td>5' 6\"</td>\n",
       "      <td>16</td>\n",
       "      <td>31.0</td>\n",
       "      <td>November 25, 2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192543</th>\n",
       "      <td>192543</td>\n",
       "      <td>fit</td>\n",
       "      <td>123612</td>\n",
       "      <td>36b</td>\n",
       "      <td>127865</td>\n",
       "      <td>155lbs</td>\n",
       "      <td>10.0</td>\n",
       "      <td>wedding</td>\n",
       "      <td>This dress was wonderful! I had originally pla...</td>\n",
       "      <td>athletic</td>\n",
       "      <td>I wore this to a beautiful black tie optional ...</td>\n",
       "      <td>gown</td>\n",
       "      <td>5' 6\"</td>\n",
       "      <td>16</td>\n",
       "      <td>30.0</td>\n",
       "      <td>August 29, 2017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>192544 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0  fit  user_id bust size  item_id  weight  rating  \\\n",
       "0                0  fit   420272       34d  2260466  137lbs    10.0   \n",
       "1                1  fit   273551       34b   153475  132lbs    10.0   \n",
       "2                2  fit   360448       NaN  1063761     NaN    10.0   \n",
       "3                3  fit   909926       34c   126335  135lbs     8.0   \n",
       "4                4  fit   151944       34b   616682  145lbs    10.0   \n",
       "...            ...  ...      ...       ...      ...     ...     ...   \n",
       "192539      192539  fit    66386      34dd  2252812  140lbs    10.0   \n",
       "192540      192540  fit   118398       32c   682043  100lbs    10.0   \n",
       "192541      192541  fit    47002       36a   683251  135lbs     6.0   \n",
       "192542      192542  fit   961120       36c   126335  165lbs    10.0   \n",
       "192543      192543  fit   123612       36b   127865  155lbs    10.0   \n",
       "\n",
       "           rented for                                        review_text  \\\n",
       "0            vacation  An adorable romper! Belt and zipper were a lit...   \n",
       "1               other  I rented this dress for a photo shoot. The the...   \n",
       "2               party  This hugged in all the right places! It was a ...   \n",
       "3       formal affair  I rented this for my company's black tie award...   \n",
       "4             wedding  I have always been petite in my upper body and...   \n",
       "...               ...                                                ...   \n",
       "192539           work                                  Fit like a glove!   \n",
       "192540           work  The pattern contrast on this dress is really s...   \n",
       "192541       everyday  Like the other DVF wraps, the fit on this is f...   \n",
       "192542        wedding  This dress was PERFECTION.  it looked incredib...   \n",
       "192543        wedding  This dress was wonderful! I had originally pla...   \n",
       "\n",
       "                body type                                     review_summary  \\\n",
       "0               hourglass                               So many compliments!   \n",
       "1       straight & narrow                            I felt so glamourous!!!   \n",
       "2                     NaN  It was a great time to celebrate the (almost) ...   \n",
       "3                    pear   Dress arrived on time and in perfect condition.    \n",
       "4                athletic                    Was in love with this dress !!!   \n",
       "...                   ...                                                ...   \n",
       "192539          hourglass       LOVE IT!!! First Item Im thinking of buying!   \n",
       "192540             petite                                           LOVE it!   \n",
       "192541  straight & narrow                    Loud patterning, flattering fit   \n",
       "192542               pear  loved this dress it was comfortable and photog...   \n",
       "192543           athletic  I wore this to a beautiful black tie optional ...   \n",
       "\n",
       "        category height  size    age         review_date  \n",
       "0         romper  5' 8\"    14   28.0      April 20, 2016  \n",
       "1           gown  5' 6\"    12   36.0       June 18, 2013  \n",
       "2         sheath  5' 4\"     4  116.0   December 14, 2015  \n",
       "3          dress  5' 5\"     8   34.0   February 12, 2014  \n",
       "4           gown  5' 9\"    12   27.0  September 26, 2016  \n",
       "...          ...    ...   ...    ...                 ...  \n",
       "192539  jumpsuit  5' 9\"     8   42.0        May 18, 2016  \n",
       "192540     dress  5' 1\"     4   29.0  September 30, 2016  \n",
       "192541     dress  5' 8\"     8   31.0       March 4, 2016  \n",
       "192542     dress  5' 6\"    16   31.0   November 25, 2015  \n",
       "192543      gown  5' 6\"    16   30.0     August 29, 2017  \n",
       "\n",
       "[192544 rows x 16 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "efb8603e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4 Dropping redundant columns\n",
    "df = df.drop(['user_id', 'item_id', 'review_text'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df0430eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64\n"
     ]
    }
   ],
   "source": [
    "#5 Check if there are any string data in 'weight' column\n",
    "if df['weight'].dtype == 'object':\n",
    "    # Remove the 'lbs' suffix and convert to float\n",
    "    df['weight'] = df['weight'].str.replace('lbs', '').astype(float)\n",
    "\n",
    "# Check the datatype of 'weight' column after conversion\n",
    "print(df['weight'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45400a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique categories for 'rented for' column:\n",
      "['vacation' 'other' 'party' 'formal affair' 'wedding' 'date' 'everyday'\n",
      " 'work' nan 'party: cocktail']\n",
      "\n",
      "Unique categories after grouping:\n",
      "['vacation' 'other' 'party' 'formal affair' 'wedding' 'date' 'everyday'\n",
      " 'work' nan]\n"
     ]
    }
   ],
   "source": [
    "#6 Check unique categories for 'rented for' column\n",
    "print(\"Unique categories for 'rented for' column:\")\n",
    "print(df['rented for'].unique())\n",
    "\n",
    "# Group 'party: cocktail' category with 'party'\n",
    "df['rented for'] = df['rented for'].replace('party: cocktail', 'party')\n",
    "\n",
    "# Check unique categories again after grouping\n",
    "print(\"\\nUnique categories after grouping:\")\n",
    "print(df['rented for'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00ac7a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64\n"
     ]
    }
   ],
   "source": [
    "#7 Check if there are any string data in 'weight' column\n",
    "if df['weight'].dtype == 'object':\n",
    "    # Remove the 'lbs' suffix and convert to float\n",
    "    df['weight'] = df['weight'].str.replace('lbs', '').astype(float)\n",
    "\n",
    "# Check the datatype of 'weight' column after conversion\n",
    "print(df['weight'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af50ccbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in each column:\n",
      "Unnamed: 0            0\n",
      "fit                   0\n",
      "bust size         18411\n",
      "weight            29982\n",
      "rating               82\n",
      "rented for           10\n",
      "body type         14637\n",
      "review_summary      347\n",
      "category              0\n",
      "height              677\n",
      "size                  0\n",
      "age                 960\n",
      "review_date           0\n",
      "dtype: int64\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'numerical_column'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mC:\\Anaconda\\Anaconda\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3653\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3652\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mC:\\Anaconda\\Anaconda\\Lib\\site-packages\\pandas\\_libs\\index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mC:\\Anaconda\\Anaconda\\Lib\\site-packages\\pandas\\_libs\\index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'numerical_column'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 8\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(missing_values)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Impute missing values with appropriate methods\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# For numerical columns, you can impute with mean, median, or mode\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumerical_column\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumerical_column\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mfillna(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumerical_column\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmedian())\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# For categorical columns, you can impute with mode\u001b[39;00m\n\u001b[0;32m     11\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_column\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_column\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mfillna(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_column\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmode()[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[1;32mC:\\Anaconda\\Anaconda\\Lib\\site-packages\\pandas\\core\\frame.py:3761\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3760\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3761\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[0;32m   3762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3763\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mC:\\Anaconda\\Anaconda\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3655\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3655\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3656\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3657\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3658\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3659\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3660\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'numerical_column'"
     ]
    }
   ],
   "source": [
    "#8 Check for missing values in each column\n",
    "missing_values = df.isnull().sum()\n",
    "print(\"Missing values in each column:\")\n",
    "print(missing_values)\n",
    "\n",
    "# Impute missing values with appropriate methods\n",
    "# For numerical columns, you can impute with mean, median, or mode\n",
    "df['numerical_column'] = df['numerical_column'].fillna(df['numerical_column'].median())\n",
    "\n",
    "# For categorical columns, you can impute with mode\n",
    "df['categorical_column'] = df['categorical_column'].fillna(df['categorical_column'].mode()[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf759119",
   "metadata": {},
   "outputs": [],
   "source": [
    "#9 Statistical summary for numerical columns\n",
    "numerical_summary = df.describe()\n",
    "print(\"Statistical summary for numerical columns:\")\n",
    "print(numerical_summary)\n",
    "\n",
    "# Statistical summary for categorical columns\n",
    "categorical_summary = df.describe()\n",
    "print(\"Statistical summary for categorical columns:\")\n",
    "print(categorical_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6268790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check statistical summary\n",
    "print(df.describe(include='all'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fb2dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#10)1it Calculate the interquartile range (IQR) for 'age' column\n",
    "Q1 = df['age'].quantile(0.25)\n",
    "Q3 = df['age'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define the lower and upper bounds to identify outliers\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Identify outliers\n",
    "outliers = df[(df['age'] < lower_bound) | (df['age'] > upper_bound)]\n",
    "\n",
    "# Print the outliers\n",
    "print(\"Outliers in the 'age' column:\")\n",
    "print(outliers)\n",
    "\n",
    "# Treat outliers\n",
    "# Replace outliers with the median value\n",
    "df['age'] = df['age'].apply(lambda x: df['age'].median() if x < lower_bound or x > upper_bound else x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60680e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#11. Check the distribution of the different categories in the column 'rented for'using appropriate plot.\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Count the occurrences of each category in 'rented for' column\n",
    "category_counts = df['rented for'].value_counts()\n",
    "\n",
    "# Create a bar plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(x=category_counts.index, y=category_counts.values)\n",
    "plt.title('Distribution of Categories in \"rented for\" Column')\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17330244",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1ebb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#12. Encode the categorical variables in the dataset.\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Initialize LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Encode categorical variables\n",
    "df['category'] = label_encoder.fit_transform(df['category'])\n",
    "\n",
    "# Display the encoded data\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ce859ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0   user_id   item_id    rating      size        age  fit  \\\n",
      "0   -1.732042 -0.274069  1.508460  0.634687  0.206575  -0.728589  fit   \n",
      "1   -1.732024 -0.781651 -1.107903  0.634687 -0.028862   0.264205  fit   \n",
      "2   -1.732006 -0.481030  0.022448  0.634687 -0.970608  10.192151  fit   \n",
      "3   -1.731988  1.419890 -1.141604 -0.763875 -0.499735   0.016007  fit   \n",
      "4   -1.731970 -1.202350 -0.532714  0.634687 -0.028862  -0.852689  fit   \n",
      "\n",
      "  bust size  weight     rented for  \\\n",
      "0       34d  137lbs       vacation   \n",
      "1       34b  132lbs          other   \n",
      "2       NaN     NaN          party   \n",
      "3       34c  135lbs  formal affair   \n",
      "4       34b  145lbs        wedding   \n",
      "\n",
      "                                         review_text          body type  \\\n",
      "0  An adorable romper! Belt and zipper were a lit...          hourglass   \n",
      "1  I rented this dress for a photo shoot. The the...  straight & narrow   \n",
      "2  This hugged in all the right places! It was a ...                NaN   \n",
      "3  I rented this for my company's black tie award...               pear   \n",
      "4  I have always been petite in my upper body and...           athletic   \n",
      "\n",
      "                                      review_summary category height  \\\n",
      "0                               So many compliments!   romper  5' 8\"   \n",
      "1                            I felt so glamourous!!!     gown  5' 6\"   \n",
      "2  It was a great time to celebrate the (almost) ...   sheath  5' 4\"   \n",
      "3   Dress arrived on time and in perfect condition.     dress  5' 5\"   \n",
      "4                    Was in love with this dress !!!     gown  5' 9\"   \n",
      "\n",
      "          review_date  \n",
      "0      April 20, 2016  \n",
      "1       June 18, 2013  \n",
      "2   December 14, 2015  \n",
      "3   February 12, 2014  \n",
      "4  September 26, 2016  \n"
     ]
    }
   ],
   "source": [
    "#13. Standardize the data, so that the values are within a particular range\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Assuming 'data' is your DataFrame\n",
    "# Select only numerical columns\n",
    "numerical_df = df.select_dtypes(include=['number'])\n",
    "\n",
    "# Initialize StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the numerical data\n",
    "scaled_df= scaler.fit_transform(numerical_df)\n",
    "\n",
    "# Convert scaled data back to DataFrame\n",
    "scaled_df = pd.DataFrame(scaled_df, columns=numerical_df.columns)\n",
    "\n",
    "# Combine with non-numeric columns from the original DataFrame\n",
    "for column in df.columns:\n",
    "    if column not in numerical_df.columns:\n",
    "        scaled_df[column] = df[column]\n",
    "\n",
    "# Display the scaled data\n",
    "print(scaled_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ea25fe8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'imputed_non_numeric_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m imputed_non_numeric_data \u001b[38;5;241m=\u001b[39m non_numeric_imputer\u001b[38;5;241m.\u001b[39mfit_transform(non_numeric_cols)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Concatenate imputed numeric and non-numeric data\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m imputed_df\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mhstack((imputed_numeric_df, imputed_non_numeric_df))\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Scale the data\u001b[39;00m\n\u001b[0;32m     28\u001b[0m scaler \u001b[38;5;241m=\u001b[39m StandardScaler()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'imputed_non_numeric_df' is not defined"
     ]
    }
   ],
   "source": [
    "# 14. Apply PCA on the above dataset and determine the number of PCAcomponents to be used so that 90-95% of the variance in data is explained bythe same\n",
    "    import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load your data into a DataFrame\n",
    "# Replace 'your_data.csv' with the path to your data file\n",
    "df = pd.read_csv(\"renttherunway.csv\")\n",
    "\n",
    "# Separate numeric and non-numeric columns\n",
    "numeric_cols = df.select_dtypes(include=np.number)\n",
    "non_numeric_cols = df.select_dtypes(exclude=np.number)\n",
    "\n",
    "# Impute missing values for numeric columns\n",
    "numeric_imputer = SimpleImputer(strategy='mean')\n",
    "imputed_numeric_df = numeric_imputer.fit_transform(numeric_cols)\n",
    "\n",
    "# Impute missing values for non-numeric columns\n",
    "non_numeric_imputer = SimpleImputer(strategy='most_frequent')\n",
    "imputed_non_numeric_data = non_numeric_imputer.fit_transform(non_numeric_cols)\n",
    "\n",
    "# Concatenate imputed numeric and non-numeric data\n",
    "imputed_df= np.hstack((imputed_numeric_df, imputed_non_numeric_df))\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "scaled_df = scaler.fit_transform(imputed_df)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA()\n",
    "pca.fit(scaled_df)\n",
    "\n",
    "# Calculate cumulative explained variance\n",
    "cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "# Find the number of components explaining 90-95% variance\n",
    "n_components_90 = np.argmax(cumulative_variance >= 0.9) + 1\n",
    "n_components_95 = np.argmax(cumulative_variance >= 0.95) + 1\n",
    "\n",
    "print(\"Number of components to explain 90% variance:\", n_components_90)\n",
    "print(\"Number of components to explain 95% variance:\", n_components_95)\n",
    "\n",
    "# Plot cumulative explained variance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o', linestyle='--')\n",
    "plt.title('Cumulative Explained Variance Plot')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.axvline(x=n_components_90, color='r', linestyle='--', label='90% Variance')\n",
    "plt.axvline(x=n_components_95, color='g', linestyle='--', label='95% Variance')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0664475",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'fit'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_25316\\3690888887.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m# Step a: Find the optimal K value using the elbow plot method\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0minertia\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m11\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mkmeans\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[0mkmeans\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m     \u001b[0minertia\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkmeans\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minertia_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m# Plot the elbow plot\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\Anaconda\\Lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1147\u001b[0m                 skip_parameter_validation=(\n\u001b[0;32m   1148\u001b[0m                     \u001b[0mprefer_skip_nested_validation\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mglobal_skip_validation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1149\u001b[0m                 )\n\u001b[0;32m   1150\u001b[0m             ):\n\u001b[1;32m-> 1151\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Anaconda\\Anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1467\u001b[0m         \u001b[1;33m-\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1468\u001b[0m         \u001b[0mself\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1469\u001b[0m             \u001b[0mFitted\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1470\u001b[0m         \"\"\"\n\u001b[1;32m-> 1471\u001b[1;33m         X = self._validate_data(\n\u001b[0m\u001b[0;32m   1472\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1473\u001b[0m             \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"csr\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1474\u001b[0m             \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\Anaconda\\Lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    600\u001b[0m                 \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    601\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    602\u001b[0m                 \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    603\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 604\u001b[1;33m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"X\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    605\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    606\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    607\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\Anaconda\\Lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    914\u001b[0m                         )\n\u001b[0;32m    915\u001b[0m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_asarray_with_order\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mxp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 918\u001b[1;33m             \u001b[1;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    919\u001b[0m                 raise ValueError(\n\u001b[0;32m    920\u001b[0m                     \u001b[1;34m\"Complex data not supported\\n{}\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    921\u001b[0m                 ) from complex_warning\n",
      "\u001b[1;32mC:\\Anaconda\\Anaconda\\Lib\\site-packages\\sklearn\\utils\\_array_api.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[0;32m    376\u001b[0m         \u001b[1;31m# Use NumPy API to support order\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    377\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcopy\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    378\u001b[0m             \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    379\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 380\u001b[1;33m             \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    381\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    382\u001b[0m         \u001b[1;31m# At this point array is a NumPy ndarray. We convert it to an array\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    383\u001b[0m         \u001b[1;31m# container that is consistent with the input's namespace.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\Anaconda\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m   1996\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnpt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDTypeLike\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1997\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1998\u001b[1;33m         \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1999\u001b[0m         if (\n\u001b[0;32m   2000\u001b[0m             \u001b[0mastype_is_view\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2001\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0musing_copy_on_write\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'fit'"
     ]
    }
   ],
   "source": [
    "#15. Apply K-means clustering and segment the data. (You may use originaldata or PCA transformed data)  \n",
    "    \n",
    "    import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load your data into a DataFrame\n",
    "# Replace 'your_data.csv' with the path to your data file\n",
    "df = pd.read_csv('renttherunway.csv')\n",
    "\n",
    "# Assuming you have selected the appropriate features for clustering\n",
    "# You can also use PCA transformed data if you have applied PCA earlier\n",
    "\n",
    "# Step a: Find the optimal K value using the elbow plot method\n",
    "inertia = []\n",
    "for k in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(df)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "# Plot the elbow plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, 11), inertia, marker='o')\n",
    "plt.title('Elbow Plot')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Inertia')\n",
    "plt.xticks(range(1, 11))\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Find the optimal K value by locating the elbow point\n",
    "optimal_k = np.argmin(inertia) + 1\n",
    "print(\"Optimal K value:\", optimal_k)\n",
    "\n",
    "# Step b: Build a K-means clustering model using the obtained optimal K value\n",
    "kmeans_model = KMeans(n_clusters=optimal_k, random_state=42)\n",
    "clusters = kmeans_model.fit_predict(df)\n",
    "\n",
    "# Step c: Compute silhouette score for evaluating the quality of the K Means clustering technique\n",
    "silhouette_avg = silhouette_score(df, clusters)\n",
    "print(\"Silhouette Score:\", silhouette_avg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5df6828",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'fit'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m sample_df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39msample(frac\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)  \u001b[38;5;66;03m# Adjust the fraction as needed\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Compute the linkage matrix using Ward's method\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m linkage_matrix \u001b[38;5;241m=\u001b[39m linkage(sample_df, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mward\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Plot the dendrogram\u001b[39;00m\n\u001b[0;32m     23\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n",
      "File \u001b[1;32mC:\\Anaconda\\Anaconda\\Lib\\site-packages\\scipy\\cluster\\hierarchy.py:1045\u001b[0m, in \u001b[0;36mlinkage\u001b[1;34m(y, method, metric, optimal_ordering)\u001b[0m\n\u001b[0;32m   1042\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m _LINKAGE_METHODS:\n\u001b[0;32m   1043\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid method: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmethod\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1045\u001b[0m y \u001b[38;5;241m=\u001b[39m _convert_to_double(np\u001b[38;5;241m.\u001b[39masarray(y, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mc\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m   1047\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1048\u001b[0m     distance\u001b[38;5;241m.\u001b[39mis_valid_y(y, throw\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mC:\\Anaconda\\Anaconda\\Lib\\site-packages\\scipy\\cluster\\hierarchy.py:1573\u001b[0m, in \u001b[0;36m_convert_to_double\u001b[1;34m(X)\u001b[0m\n\u001b[0;32m   1571\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_convert_to_double\u001b[39m(X):\n\u001b[0;32m   1572\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39mdouble:\n\u001b[1;32m-> 1573\u001b[0m         X \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mdouble)\n\u001b[0;32m   1574\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m X\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mcontiguous:\n\u001b[0;32m   1575\u001b[0m         X \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mcopy()\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'fit'"
     ]
    }
   ],
   "source": [
    "#16. Apply Agglomerative clustering and segment the data. (You may use\n",
    "#original data or PCA transformed data)\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load your data into a DataFrame\n",
    "# Replace 'your_data.csv' with the path to your data file\n",
    "df = pd.read_csv(\"renttherunway.csv\")\n",
    "\n",
    "# Assuming you have selected the appropriate features for clustering\n",
    "# You can also use PCA transformed data if you have applied PCA earlier\n",
    "\n",
    "# Step a: Find the optimal K value using dendrogram for Agglomerative clustering\n",
    "# Take a sample of the dataset to reduce computational time\n",
    "sample_df = df.sample(frac=0.5, random_state=42)  # Adjust the fraction as needed\n",
    "\n",
    "# Compute the linkage matrix using Ward's method\n",
    "linkage_matrix = linkage(sample_df, method='ward')\n",
    "\n",
    "# Plot the dendrogram\n",
    "plt.figure(figsize=(12, 6))\n",
    "dendrogram(linkage_matrix)\n",
    "plt.title('Dendrogram for Agglomerative Clustering')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Distance')\n",
    "plt.show()\n",
    "\n",
    "# Step b: Build an Agglomerative clustering model using the obtained optimal K value observed from the dendrogram\n",
    "# Visual inspection of the dendrogram can help determine the optimal number of clusters (K)\n",
    "kmeans_model = KMeans(n_clusters=optimal_k, random_state=42)\n",
    "clusters = kmeans_model.fit_predict(df)\n",
    "\n",
    "\n",
    "# Step c: Compute silhouette score for evaluating the quality of the Agglomerative clustering technique\n",
    "# Assuming you have determined the optimal K value manually from the dendrogram\n",
    "optimal_k = 3  # Adjust this based on dendrogram observation\n",
    "\n",
    "# Build Agglomerative clustering model\n",
    "agg_model = AgglomerativeClustering(n_clusters=optimal_k)\n",
    "clusters = agg_model.fit_predict(df)\n",
    "\n",
    "# Compute silhouette score\n",
    "silhouette_avg = silhouette_score(df, clusters)\n",
    "print(\"Silhouette Score:\", silhouette_avg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2368b551",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'cluster_labels'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mC:\\Anaconda\\Anaconda\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3653\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3652\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mC:\\Anaconda\\Anaconda\\Lib\\site-packages\\pandas\\_libs\\index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mC:\\Anaconda\\Anaconda\\Lib\\site-packages\\pandas\\_libs\\index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'cluster_labels'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 12\u001b[0m\n\u001b[0;32m      8\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrenttherunway.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Assuming you have performed K-means clustering and assigned cluster labels to your dataset\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Replace 'cluster_labels' with the actual column name containing cluster labels\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m cluster_labels \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcluster_labels\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Assuming you have other features in your dataset\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Replace 'feature1', 'feature2', ... with actual feature names\u001b[39;00m\n\u001b[0;32m     16\u001b[0m features \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]\n",
      "File \u001b[1;32mC:\\Anaconda\\Anaconda\\Lib\\site-packages\\pandas\\core\\frame.py:3761\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3760\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3761\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[0;32m   3762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3763\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mC:\\Anaconda\\Anaconda\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3655\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3655\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3656\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3657\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3658\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3659\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3660\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'cluster_labels'"
     ]
    }
   ],
   "source": [
    "#17. Perform cluster analysis by doing bivariate analysis between cluster labels\n",
    "#and different features and write your conclusion on the results. \n",
    " \n",
    "    import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import f_oneway\n",
    "\n",
    "# Load your data into a DataFrame\n",
    "# Replace 'your_data.csv' with the path to your data file\n",
    "df = pd.read_csv(\"renttherunway.csv\")\n",
    "\n",
    "# Assuming you have performed K-means clustering and assigned cluster labels to your dataset\n",
    "# Replace 'cluster_labels' with the actual column name containing cluster labels\n",
    "cluster_labels = df['cluster_labels']\n",
    "\n",
    "# Assuming you have other features in your dataset\n",
    "# Replace 'feature1', 'feature2', ... with actual feature names\n",
    "features = ['feature1', 'feature2', ...]\n",
    "\n",
    "# Bivariate analysis for each feature\n",
    "for feature in features:\n",
    "    # Box plot or violin plot to visualize feature distribution across clusters\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.boxplot(x=cluster_labels, y=df[feature])\n",
    "    plt.title(f'{feature} by Cluster')\n",
    "    plt.xlabel('Cluster Labels')\n",
    "    plt.ylabel(feature)\n",
    "    plt.show()\n",
    "    \n",
    "    # Statistical test (ANOVA) to determine if there are significant differences in feature distributions\n",
    "    f_statistic, p_value = f_oneway(*(df[feature][cluster_labels == label] for label in cluster_labels.unique()))\n",
    "    print(f\"ANOVA p-value for {feature}: {p_value}\")\n",
    "\n",
    "# Conclusion\n",
    "# Based on the visualizations and statistical tests:\n",
    "# - Identify features that show significant differences in distributions across clusters (low p-value).\n",
    "# - Interpret how these features contribute to cluster differentiation.\n",
    "# - Draw conclusions about the characteristics of each cluster based on the features analyzed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57dc30c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
